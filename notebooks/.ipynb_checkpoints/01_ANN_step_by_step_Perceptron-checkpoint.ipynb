{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We talked about the basics of Artificial Neural Networks (ANNs), gave an intuitive summary, and provided an example in the slides. In this notebook, we will continue to explore more of the ANN and implement a simple version of it.  \n",
    "\n",
    "After reading this notebook, you will understand some key concepts including the: *Inputs, Weights, Outputs, Targets, Activation Functions, Error, Bias term, Learning rate.*   \n",
    "\n",
    "Let's start with the simplest type of Artificial Neural Network, the [Perceptron](https://en.wikipedia.org/wiki/Perceptron). Developed back to the late 1950s, this was one of the first artificial neural networks to be produced. You can think of a perceptron as a two layer neural network without any hidden layers. Even though it has limitations, it contains the essential components found in ANNs.   \n",
    "\n",
    "Let's explore how a percepton works using a simple dataset with 5 observations where each sample has 3 features. The target is what we are ultimately trying to predict from the features. Our target is either 0 or 1 representing the different classes each sample can be, i.e. class 0, and class 1. Our goal is to build and train a simple Perceptron model that can **output the correct target** by feeding it our 3 **features as input**. See the following table. This example is modified from this [great post](http://iamtrask.github.io/2015/07/12/basic-python-network/). \n",
    "\n",
    "\n",
    "|Observation||Feature1|Feature2|Feature3||Target|\n",
    "|:------:||:------:|:------:|:------:||:----:|\n",
    "| 1 ||    0   |    0   |    1   ||   0  |\n",
    "| 2 ||    1   |    1   |    1   ||   1  |\n",
    "| 3 ||    1   |    0   |    1   ||   1  |\n",
    "| 4 ||    0   |    1   |    1   ||   0  |\n",
    "| 5 ||    0   |    1   |    0   ||   1  |\n",
    "\n",
    "Let's have a look at the structure of our model.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/qingkaikong/blog/master/39_ANN_part2_step_by_step/figures/figure1_perceptron_structure.jpg\" width=\"600\"/>  \n",
    "\n",
    "From the figure, we can see that we have two layers in this model: the input layer and the output layer. The input layer has 3 features that connect to the output layer via 3 weights.  The steps we will take to train this model are:\n",
    "1. Initialize the weights to small random numbers with both positive or negative values.\n",
    "2. For many iterations:  \n",
    "    * Calculate the output value based on all data samples.<br />\n",
    "    * Update the weights based on the error.\n",
    "\n",
    "Before we implement this in code, we need go through some concepts:  \n",
    "\n",
    "### Bias term\n",
    "Now let's look at the extra 1 we added to the input layer. It is connected to the output layer by weight $\\omega_0$. Why do we add a 1 to the input? Consider the case when all our features are 0, and no matter what weights we have, we will always have 0 as output. Adding this 1 extra node can avoid this problem. The bias term fuctions just like a y-intercept in regression, allowing us to shift our function from being rooted at 0. \n",
    "\n",
    "### Activation functions   \n",
    "The output layer only has one node that sums all the passed input and determines whether the output should fire or not (this is ANN terminology related to the brain and the firing of neurons). Usually we use a 1 to indicate a neuron firing and a 0 to indicate a neuron not firing. In this case, we can see the sum of our weighteded inputs denoted by $z$ are: \n",
    "\n",
    "$$z = 1*\\omega_0 + feature1*\\omega_1 + feature2*\\omega_2 + feature3*\\omega_3$$\n",
    "\n",
    "But what we want as the output is either 0 or 1 to represent two classes. Since this number z can be anything, how can we scale it to a value between 0 and 1? \n",
    "\n",
    "For our use case, we will use the sigmoid activation function given these three major advantages:\n",
    "1. The output is scaled between 0 and 1.\n",
    "    - If the z value we is a large positive number, say 10, the output will be close to 1 or while for very negative numbers the output will be close to 0. The closer to either extreme, the more confidence our network has on its estimation.\n",
    "2. The output is given as a probability.\n",
    "    - If we have z relatively close to 0, then the scaled value will be around 0.5, which  indicates the network is not very confident in its estimation.\n",
    "3. We can take the derivative of the output.\n",
    "    -  Since the derivative can be used to update the weights while training our model, this property from the sigmoid function lets our shift the weights to predict the correct target.\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1868/1*9-FQ4ZfGyoYT36BTDh0hZg.png\" width=\"600\"/> \n",
    "\n",
    "So now we have a perceptron network that can take some input of features, apply some weight to each feature, sum that up and pass it through an activation function to classify an observation into two different classes, great!\n",
    "\n",
    "But what if the result wrong? That is not only possible, but **expected** since we initialized the weights as small **random** numbers. To update our model, we need the perceptron to have the ability to learn from the data (*more specifically, from its errors*). \n",
    "\n",
    "### How to learn from the error  \n",
    "\n",
    "Learning will be achieved by:\n",
    "1. **Estimating our error** from the current weights\n",
    "2. Finding a way to update the weights that will **reduce the error**.\n",
    "\n",
    "### Learning rate   \n",
    "\n",
    "*Why do we need a learning rate?* - The weights change a lot when the error is high, and we may overshoot our weight updates leading to another extreme, making the network unstable so that it never settles down. \n",
    "\n",
    "For this reason, wen updating our weights, we use a learning rate to control how fast the network learns. The learning rate is hyperparameter we set between 0-1 that is multiplied by our weight updates to reduce how much we change the weights by (if set to 1, it has no effect on the weight). \n",
    "\n",
    "There is a trade off in setting the learning rate too high or too low. A high learning rate makes training our ANN fast, but can cause it to jump past the optimal weights. On the other hand a small learning rate can lead to more stable weights but make training take longer or potentially get stuck in local minima. \n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_learning_rate.png\" width=\"600\" />\n",
    "\n",
    "Let's implement what we discussed above, explaining them line by line below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Before training: \n",
      "[[-0.17]\n",
      " [ 0.44]\n",
      " [-1.  ]\n",
      " [-0.4 ]]\n",
      "Outputs Before Training:\n",
      "[[0.36]\n",
      " [0.25]\n",
      " [0.47]\n",
      " [0.17]\n",
      " [0.24]]\n",
      "Targets\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "-------------------------\n",
      "Weights After Training:\n",
      "[[  5.69]\n",
      " [ 16.6 ]\n",
      " [  1.5 ]\n",
      " [-14.52]]\n",
      "Outputs After Training:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Targets\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Takes a number z (the weighted sum of inputs)\n",
    "    Returns the output of the sigmoid function evaluted at z.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# the slope (derivative) of the sigmoid function\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"\n",
    "    Takes an input number z (the weighted sum of inputs)\n",
    "    Returns the derivative of the sigmoid with respect to z.\n",
    "    \"\"\"\n",
    "    sig = sigmoid(z)\n",
    "    return sig*(1-sig)\n",
    "    \n",
    "\n",
    "# define learning rate\n",
    "learning_rate = 0.4\n",
    "\n",
    "# input dataset with each observation as an array\n",
    "X = np.array([[0,0,1],\n",
    "              [1,1,1],\n",
    "              [1,0,1],\n",
    "              [0,1,1],\n",
    "              [0,1,0]])\n",
    "              \n",
    "# we add a bias column of 1s to the input\n",
    "X = np.concatenate((np.ones((len(X), 1)), X), axis = 1)\n",
    "\n",
    "# true target for each data sample     \n",
    "y = np.array([[0,1,1,0,1]]).T\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly between [-1, 1] with a mean 0\n",
    "weights_0 = (2 * np.random.random((4,1))) - 1\n",
    "\n",
    "print(\"Weights Before training: \")\n",
    "print(weights_0.round(2))\n",
    "print(\"Outputs Before Training:\")\n",
    "print(sigmoid(np.dot(X,weights_0)).round(2))\n",
    "print('Targets')\n",
    "print(y)\n",
    "\n",
    "# store errors at each iteration to track learning              \n",
    "errors=[]\n",
    "\n",
    "# train the network with 50,000 iterations\n",
    "for iter in range(50000):\n",
    "\n",
    "    # forward propagation\n",
    "    layer_0 = X\n",
    "              \n",
    "    # pass the weighted sum of the inputs through the sigmoid activation function\n",
    "    layer_1_output = sigmoid(np.dot(layer_0,weights_0))\n",
    "\n",
    "    # calculate the error (target value - predicted value)\n",
    "    layer1_error = y - layer_1_output\n",
    "              \n",
    "    # store error of current iteration\n",
    "    errors.append(np.sum(layer1_error))\n",
    "              \n",
    "    # compute the change (delta) for each sample by multipying \n",
    "    # 1. the learning rate \n",
    "    # 2. error\n",
    "    # 3. slope (derivative of our the sigmoid with respect to the weighted sum of inputs (layer_1_output))\n",
    "    layer1_delta = learning_rate * layer1_error * sigmoid_derivative(layer_1_output)\n",
    "    \n",
    "    # compute the change (delta) for each weight\n",
    "    # we multiply by our input (transpose for the correct shape), x\n",
    "    layer1_delta = np.dot(layer_0.T,layer1_delta)\n",
    "    \n",
    "    # lastly we update weights by simply adding the delta \n",
    "    weights_0 += layer1_delta\n",
    "print(\"-------------------------\")    \n",
    "print(\"Weights After Training:\")\n",
    "print(weights_updated.round(2))\n",
    "print(\"Outputs After Training:\")\n",
    "print(layer_1_output.round(2))\n",
    "print('Targets')\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualizing the errors over time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_errors(errors):\n",
    "    \"\"\"\n",
    "    Plots the total error over time (each iteration of weight updates)\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title('Perceptron Total Error Over Each Iteration of Learning')\n",
    "    plt.plot(errors)\n",
    "    plt.ylabel('Total Error')\n",
    "    plt.xlabel('Training iteration')\n",
    "    plt.show()\n",
    "    \n",
    "plot_errors(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain line by line  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Line 1:** This line imports the numpy module, which is a linear algebra library.   \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "**Line 3:** These lines define the activation function, which is a function to convert any number to a probability between 0 and 1 as we discussed above and the derivative (or slope) of the sigmoid with respect to the weighted sum z.   \n",
    "\n",
    "```python\n",
    "# the sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Takes a number z (the weighted sum of inputs)\n",
    "    Returns the output of the sigmoid function evaluted at z.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# the slope (derivative) of the sigmoid function\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"\n",
    "    Takes an input number z (the weighted sum of inputs)\n",
    "    Returns the derivative of the sigmoid with respect to z.\n",
    "    \"\"\"\n",
    "    sig = sigmoid(z)\n",
    "    return sig*(1-sig)\n",
    "```\n",
    "\n",
    "**Line 22:** Here we define our learning rate, this will control how fast the network learns from the data. Usually this learning rate will be a number between 0 - 1.\n",
    "\n",
    "```python\n",
    "# define the learning rate\n",
    "learning_rate = 0.4\n",
    "```\n",
    "\n",
    "**Line 25:** This initializes the input dataset as a numpy matrix. Each row is a single data sample, and each column corresponds to one features (one of the input nodes). We then add the bias term, which is a column of ones. You can see that we now have 4 input nodes and 5 training examples.   \n",
    "\n",
    "```python\n",
    "# input dataset with each observation as an array\n",
    "X = np.array([[0,0,1],\n",
    "              [1,1,1],\n",
    "              [1,0,1],\n",
    "              [0,1,1],\n",
    "              [0,1,0]])\n",
    "# we add a bias column of 1s to the input\n",
    "X = np.concatenate((np.ones((len(X), 1)), X), axis = 1)\n",
    "```\n",
    "\n",
    "**Line 35:** Define our target, the true value of each data sample. \".T\" is the transpose function, which converts our output data to a column vector. \n",
    "\n",
    "```python\n",
    "# true target for each data sample          \n",
    "y = np.array([[0,1,1,0,1]]).T\n",
    "```\n",
    "\n",
    "**Line 38:** Before we generate the random weights, we set a seed to ensure we can replicate our experiment by drawing the same random numbers. This is very useful when we test the algorithm, and compare our results with others. \n",
    "\n",
    "```python\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "```\n",
    "\n",
    "**Line 41:** This initializes our weights that connect the input layer to the output layer. Since we have 4 input features (including the bias term), we initialize the random weights as four rows with one column - dimensions (4,1). Also note that the random numbers we initialized are within -1 to 1, with a mean of 0. There is quite a bit of theory that goes into the weight initialization but for now we will simply ensure to have a mean of 0 for weight initialization.\n",
    "\n",
    "```python\n",
    "# initialize weights randomly between [-1, 1] with a mean 0\n",
    "weights_0 = 2*np.random.random((4,1)) - 1\n",
    "```\n",
    "\n",
    "**Line 44:** Keep track of our weights, outputs and targets before training the ANN.\n",
    "\n",
    "```python\n",
    "print(\"Weights Before training: \")\n",
    "print(weights_0.round(2))\n",
    "print(\"Outputs Before Training:\")\n",
    "print(sigmoid(np.dot(X,weights_0)).round(2))\n",
    "print('Targets')\n",
    "print(y)\n",
    "print(\"Weights Before training: \")\n",
    "print(weights_0.round(2))\n",
    "print(\"Outputs Before Training:\")\n",
    "print(sigmoid(np.dot(X,weights_0)).round(2))\n",
    "print('Targets')\n",
    "print(y)\n",
    "```\n",
    "\n",
    "**Line 51:** Keep track of our error through each iteration of learning.\n",
    "\n",
    "```python\n",
    "# store errors at each iteration to track learning              \n",
    "errors=[]\n",
    "```\n",
    "\n",
    "**Line 54:** We train our model with 50,000 iterations. In each iteration, we update our weights.  \n",
    "\n",
    "```python\n",
    "# train the network with 50,000 iterations\n",
    "for iter in xrange(50000):\n",
    "```\n",
    "\n",
    "**Line 57:** We explicitly assign our input data as layer_0. We're going to process all the data at the same time in this implementation, this is called 'batch processing'. \n",
    "\n",
    "```python\n",
    "    # forward propagation\n",
    "    layer_0 = X\n",
    "```\n",
    "\n",
    "**Line 60:** This is forward propagation. It has two steps\n",
    "\n",
    "1. Take the dot product of the raw inputs and the weights using the np.dot function:\n",
    "\n",
    "$$z = np.dot(layer_0,weights_0) = 1*\\omega_0 + feature1*\\omega_1 + feature2*\\omega_2 + feature3*\\omega_3$$\n",
    "\n",
    "2. Pass the weighted sum, $z$, to the sigmoid function to get an output between 0 to 1.\n",
    "\n",
    "$$output = sigmoid(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "```python\n",
    "    # pass the weighted sum of the inputs through the sigmoid activation function\n",
    "    layer_1_output = sigmoid(np.dot(layer_0,weights_0))\n",
    "```\n",
    "\n",
    "**Line 63:** This is the error for our current weights. It is simply the target (y) minus our estimation/output (layer_1_output). The error of each observation is stored here as a 5 by 1 matrix.  \n",
    "\n",
    "```python\n",
    "    # calculate the error (target value - predicted value)\n",
    "    layer1_error = y - layer_1_output\n",
    "```\n",
    "\n",
    "**Line 69:** This is the most important part - learning. This is the line that makes our algorithm learn from the data. It calculates how much we will change each of the weights in the current iteration for a given datapoint. It has 3 parts that are multiplied together, (1) the learning rate, (2) the current error, and (3) the slope (derivative) of the sigmoid function.\n",
    "\n",
    "```python\n",
    "    # compute the change (delta) for each sample by multipying \n",
    "    # 1. the learning rate \n",
    "    # 2. error\n",
    "    # 3. slope (derivative of our the sigmoid with respect to the weighted sum of inputs (layer_1_output))\n",
    "    layer1_delta = learning_rate * layer1_error * sigmoid_derivative(layer_1_output)\n",
    "```\n",
    "\n",
    "1. The first term is the learning rate, which controls how fast we will learn. It is just a constant value between 0-1, and this case, we chose 0.4. \n",
    "\n",
    "2. The second term is the errors for each of our observations. The higher the error, the more we update our weights.\n",
    "\n",
    "3. The final term is the derivative of the sigmoid function evaluated at the output value. We can see a simple plot of the sigmoid function and its derivatives at different places. \n",
    "     - In the figure below, we can see the derivative at x is the slope of the sigmoid curve at value x. The more uncertain our output (say the blue dot at 0.5) the steeper the slope (blue line), and more we update our weights. The more certain our ouput (the green dot) the flatter the slope (green line) and less we update our weights.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/qingkaikong/blog/master/39_ANN_part2_step_by_step/figures/figure3_sigmoid_derivative.jpg\" width=\"600\"/> \n",
    "<img src=\"https://i.stack.imgur.com/inMoa.png\" width=\"600\"/>\n",
    "\n",
    "**Line 75:** After calculating our updates with respect to each example, we need to calculate the change with respect to each weight. We do this by multiplying by the input.   \n",
    "\n",
    "```python\n",
    "    # compute the change (delta) for each weight\n",
    "    # we multiply by our input (transpose for the correct shape), x\n",
    "    layer1_delta = np.dot(layer_0.T,layer1_delta)\n",
    "```\n",
    "\n",
    "**Line 79:** This is the last line, it aims to update the current weights by simply adding the delta we just determined to the current weights.  \n",
    "\n",
    "```python\n",
    "    # update weights by simply adding the delta\n",
    "    weights_0 += layer1_delta\n",
    "```\n",
    "\n",
    "**Line 81:** These lines print out the final updated weights and outputs after 50,000 iterations, along with the true targets, y. As you can see, we get great results!   \n",
    "\n",
    "```python\n",
    "print(\"-------------------------\")    \n",
    "print(\"Weights After Training:\")\n",
    "print(weights_updated.round(2))\n",
    "print(\"Outputs After Training:\")\n",
    "print(layer_1_output.round(2))\n",
    "print('Targets')\n",
    "```\n",
    "\n",
    "This concludes this part by showing you all the important part of the ANN (perceptron) algorithm. But the perceptron has its own limation, which actually caused the ANN winter we talked about in the 1970s until researchers found the way to solve it - the multilayer perceptron algorithm, which we will talk in the next part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Advanced Topics\n",
    "\n",
    "Think of neural network as a bicycle with exchangeable parts. A few of the parts of a neural network we can swap out include:\n",
    "1. The Activation Function\n",
    "2. The Cost Function\n",
    "3. The Weight Updates\n",
    "4. The Architecture\n",
    "\n",
    "For our simple perceptron we built a neural network with:\n",
    "1. The Activation Function - **sigmoid**\n",
    "2. The Cost Function - **$target-prediction$**\n",
    "3. The Weight Updates - **$learning rate*error*slope*input$**\n",
    "4. The Architecture - **2-layers, input and output**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Activation Functions\n",
    "\n",
    "While we used the sigmoid activation function for this notebook, there are many different types of activation functions used in practice. These activation functions each have their own tradeoffs. Below are a collection of different activation functions, and a brief description of what they aim to do, and their drawbacks:\n",
    "\n",
    "1. **Sigmoid (Logistic)** - Can squish output between 0-1, but neurons can die from oversaturation of the gradients (vanishing gradients) during backpropagation (the derivative is near 0 when the input is very high or low!) and high computational overhead to calculate $e^-x$.\n",
    "\n",
    "2. **Tanh (Hyperbolic Tangent)** - Very similar to the sigmoid, but can squish output between -1-1 and is 0-centered (allowing backpropagation to be more efficient) but still has the issues of oversaturation of the gradients and high computational overhead.\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit)** - Very computationally efficient and the gradient doesn't oversaturate at the positive end, but neurons die when the input is $<= 0$ (the derivative (slope) is 0 when the input is $<= 0$ ). \n",
    "\n",
    "4. **Leaky ReLU (Leaky Rectified Linear Unit)** - Similar to ReLU, but neurons will not die when the input is $<= 0$ since the derivative is not 0 at these points, but instead the $input*0.1$\n",
    "5. **Maxout** - A generalization of ReLU and Leaky ReLU that doesn't kill neurons, but requires double the number of parameters for each neuron. \n",
    "6. **ELU (Exponential Linear Unit)** - Similar to Leaky ReLU, but requires high computational overhead to calculate $e^-x$. \n",
    "7. Many more!\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2384/0*sIJ-gbjlz0zrz8lb.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 & 3. The Cost Function and Weight Updates\n",
    "\n",
    "In our code above, we developed a perceptron that learns how to map inputs to targets by **reducing the errors** and **changing the weights**. \n",
    "\n",
    "If you understand that, fantastic! You have a good overview how neural networks **learn**. But can we reach a bit deeper to understand *how* specific changes to weights will make our perceptron better? Below, we will see why our weight update rule\n",
    "\n",
    "$$\\text{weights}_{new}= \\text{weights}_{old} + \\text{learning rate}*\\text{error}*\\text{slope}*\\text{input}$$\n",
    "\n",
    "makes sense from a mathematical perspective.\n",
    "\n",
    "***\n",
    "\n",
    "#### <center> Math alert! - For completeness we will review the underlying math that governs how Artificial Neural Networks learn. </center>\n",
    "\n",
    "In ANNs and machine learning in general, the objective of our algorithms are to minimize the value of a **Cost Function** (mimizing cost = much better predictions!). In our context, we want to *minimize the cost function (i.e. the prediction error) by only changing the weights of our ANN*.\n",
    "\n",
    "Consider the cost function $C$ which takes two arugments $y$, and $\\hat{y}$ target (i.e. label) and the model output (i.e. model prediction) respectively:\n",
    "\n",
    "$$C(y,\\hat{y}) = \\frac{1}{2}(y-\\hat{y})^{2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 & 3. The Cost Function and Weight Updates\n",
    "\n",
    "\n",
    "\n",
    "So our goal is to *minimize the cost function (i.e. the prediction error) by only changing the weights of our ANN*.\n",
    "\n",
    "In ANNs and machine learning in general, the objective of our algorithms are to minimize the value of the cost function (mimizing cost = much better predictions!). \n",
    "\n",
    "We used the following formula to update our weights: \n",
    "\n",
    "$$\\text{weights}_{new}= \\text{weights}_{old} + \\text{learning rate}*\\text{error}*\\text{slope}*\\text{input}$$\n",
    "\n",
    "Before going into mathland, imagine that we had a magical way to know how much would the error change if we increased a weight by a really tiny amount, say 0.000000001. In math, we would denote than as,\n",
    "\n",
    "$$\\frac{\\text{change in error}}{\\text{tiny change in weight}} = \\frac{\\partial C}{\\partial w}$$\n",
    "\n",
    "So, let's say that we know that whenever we increase a weight by a reall small amount, the error increases by 1. We would denote that as\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w} = 1$$\n",
    "\n",
    "Now stop. What should we do??? \n",
    "\n",
    "- Increase the weight\n",
    "- Decrease the weight\n",
    "\n",
    "Really, think about it...\n",
    "\n",
    "We would decrease the weight! Otherwise, the error increases, which makes us sad.\n",
    "\n",
    "Ok, cool. You get it. Now, how exactly do we get the \"sensitivity\" of the error to a tiny change in weights?\n",
    "\n",
    "#### Finding the magical sensitivity of the error to changes in weights\n",
    "\n",
    "Consider the cost function $C$ which takes two arugments $y$, and $\\hat{y}$ which represent the target (i.e. label) and the model output (i.e. model prediction) respectively:\n",
    "\n",
    "$$C(y,\\hat{y}) = \\frac{1}{2}(y-\\hat{y})^{2}$$\n",
    "\n",
    "Notice that the model output (i.e. the model predictions) are the result of the sigmoid function $\\sigma(z)$\n",
    "\n",
    "$$\\hat{y} = \\sigma(z) = \\frac{1}{(1+e^{-z})}$$\n",
    "\n",
    "And as it is stated, $\\sigma(z)$ depends on our summation term, $z$, which is\n",
    "\n",
    "$$z(x,w) = x_0*w_0 + x_1*w_1 + x_2*w_2 +x_3*w_3$$\n",
    "\n",
    "There is a butterfly effect where the weights we want to update are passed through several functions before we can see its effect on the cost funcion... \n",
    "\n",
    "Utilizing the chain rule, we can derive that:\n",
    "$$\\frac{\\partial C}{\\partial w_i} = \\frac{\\partial C}{\\partial o} \\frac{\\partial o}{\\partial z} \\frac{\\partial z}{\\partial w_i}$$\n",
    "\n",
    "Skipping the full derivation, we can calculate each component:\n",
    "$$\\frac{\\partial C}{\\partial o} = -(t-o)\\\\\n",
    "\\frac{\\partial o}{\\partial z} = o(1-o)\\\\\n",
    "\\frac{\\partial z}{\\partial w_i} = x_i$$\n",
    "\n",
    "Does this look familiar? Each component corresponds to what we previously calculated to update our weights!\n",
    "1. $\\frac{\\partial C}{\\partial o} = -(t-o)$ - This is our **error**\n",
    "2. $\\frac{\\partial o}{\\partial z} = o(1-o)$ - This is our **slope**\n",
    "3. $\\frac{\\partial z}{\\partial w_i} = x_i$ - This is our **input**\n",
    "\n",
    "You may have noticed that our sign for the error is flipped. This is because in practice, you subtract the derivitive of the cost function, with respect to each weight (gradient) by the current weight! Substracting a negative number is the equivalent of adding!\n",
    "\n",
    "$$weight_{new} = weight_{old} - l * \\frac{\\partial C}{\\partial w}$$\n",
    "\n",
    "This is a lot to take in, but you just learned the underlying mathetmatics of work-engine of neural networks, learning through backpropagation. Congratulations!\n",
    "Our overall goal is to *minimize the cost function by only change the weights in our ANN*.\n",
    "\n",
    "Consider the cost function $C$ which takes two arugments $(t,o)$ which represent the target and output respectively:\n",
    "\n",
    "$$C(t,o) = \\frac{1}{2}(t-o)^{2}$$\n",
    "\n",
    "Which is dependent on our sigmoid function, $o$:\n",
    "\n",
    "$$o(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Which id also dependent on our summation term, $z$\n",
    "\n",
    "$$z(x,w) = x_0*w_0 + x_1*w_1 + x_2*w_2 +x_3*w_3$$\n",
    "\n",
    "There is a butterfly effect where the weights we want to update are passed through several functions before we can see its effect on the cost funcion... \n",
    "\n",
    "This is where we can the chain rule come to the rescue! If we expand our cost function, we show that that is it is a deeply nested function\n",
    "\n",
    "$$C(t,o) = \\frac{1}{2}(t- o(z(x,w)))^{2}$$\n",
    "\n",
    "Utilizing the chain rule, we can derive that:\n",
    "$$\\frac{\\partial C}{\\partial w_i} = \\frac{\\partial C}{\\partial o} \\frac{\\partial o}{\\partial z} \\frac{\\partial z}{\\partial w_i}$$\n",
    "\n",
    "Skipping the full derivation, we can calculate each component:\n",
    "$$\\frac{\\partial C}{\\partial o} = -(t-o)\\\\\n",
    "\\frac{\\partial o}{\\partial z} = o(1-o)\\\\\n",
    "\\frac{\\partial z}{\\partial w_i} = x_i$$\n",
    "\n",
    "Does this look familiar? Each component corresponds to what we previously calculated to update our weights!\n",
    "1. $\\frac{\\partial C}{\\partial o} = -(t-o)$ - This is our **error**\n",
    "2. $\\frac{\\partial o}{\\partial z} = o(1-o)$ - This is our **slope**\n",
    "3. $\\frac{\\partial z}{\\partial w_i} = x_i$ - This is our **input**\n",
    "\n",
    "You may have noticed that our sign for the error is flipped. This is because in practice, you subtract the derivitive of the cost function, with respect to each weight (gradient) by the current weight! Substracting a negative number is the equivalent of adding!\n",
    "\n",
    "$$weight_{new} = weight_{old} - l * \\frac{\\partial C}{\\partial w}$$\n",
    "\n",
    "This is a lot to take in, but you just learned the underlying mathetmatics of work-engine of neural networks, learning through backpropagation. Congratulations!\n",
    "\n",
    "\n",
    "The aforementioned idea of  3 major ideas in ANNs:\n",
    "1. **Gradient Descent**\n",
    "2. **Backpropogation**\n",
    "3. **The Chain Rule**\n",
    "\n",
    "**Gradient descent** is an algorithm that find the mimimum of a function by calculating the slope (derivative) of our function and moving downhill towards the minimum.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/736/1*e88JKNWAFok3vpjeuPfHig.gif\" width=\"300\"/>\n",
    "\n",
    "**Backpropagation** is the ANN implementation of gradient descent to \"propagate\", or carry errors from output to input to determine what weight changes are best to minimze our cost function.\n",
    "\n",
    "**The Chain Rule** is the way compute the direction and magnitude that we must change our weights to minimize our cost function (in calculus talk: finding the partial derivative of the Cost function with respect to the weights). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Architecture\n",
    "\n",
    "We used the simplest neural network architecture in this notebook, but in practice we can add many more \"hidden layers\" with which our input passes through before obtaining our final output. This idea gave birth to the term *deep neural network* to describe an ANN with many hidden layers.\n",
    "\n",
    "In the next notebook we will explore adding hidden layers to our ANN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References  \n",
    "\n",
    "##### ANN Overview\n",
    "[A Neural Network in 11 lines of Python](http://iamtrask.github.io/2015/07/12/basic-python-network/) (I thank the author, since my example is modified from his blog).    \n",
    "[Machine learning - An Algorithmic Perspective](https://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html)\n",
    "***\n",
    "##### Activation Functions\n",
    "[Activation functions and when to use what](https://medium.com/@himanshuxd/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e)\n",
    "***\n",
    "##### Learning Rate\n",
    "[Single-Layer Neural Networks and Gradient Descent](http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html)     \n",
    "[Learning rate optimization](https://www.jeremyjordan.me/nn-learning-rate/)     \n",
    "***\n",
    "##### Backpropogation\n",
    "[Backpropagation - 3Blue1Brown](https://www.youtube.com/watch?v=Ilg3gGewQ5U&ab_channel=3Blue1Brownhttps://www.youtube.com/watch?v=Ilg3gGewQ5U&ab_channel=3Blue1Brown)     \n",
    "[Backpropgation Computation Examples - Standford Lecture](https://www.youtube.com/watch?v=d14TUNcbn1k&ab_channel=StanfordUniversitySchoolofEngineering)\n",
    "***\n",
    "##### Gradient Descent\n",
    "[Gradient Descent - 3Blue1Brown](https://www.youtube.com/watch?v=IHZwWFHWa-w&t=960s&ab_channel=3Blue1Brownhttps://www.youtube.com/watch?v=IHZwWFHWa-w&t=960s&ab_channel=3Blue1Brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
