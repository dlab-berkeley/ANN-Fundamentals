{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement another neural network using the popular machine learning package for neural networks and deep learning, [Keras](https://keras.io) which is built on top of [tensorflow](https://www.tensorflow.org).   \n",
    "\n",
    "We work with the [Handwritten Digits Data Sets](http://scikit-learn.org/stable/datasets/#optical-recognition-of-handwritten-digits-data-set) again to classify the digits using a Multilayer Perceptron (MLP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the needed module\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize data \n",
    "\n",
    "Just like in the last notebook, let's load the [Handwritten Digits Data Set](http://yann.lecun.com/exdb/mnist/) but from Keras this time. Once again, the dataset contains images of the 10 classes of handwritten digits (0 to 9). The data includes a training set of 60,000 examples, and a test set of 10,000 examples which is a subset of a larger set available from NIST.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data which is already split for us as a train and test set\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The dataset contains {len(x_train)} training data examples, and {len(x_test)} test data examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unique classes\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of anti-aliasing (a technique to minimize distortion when compressing an image). The images were also centered in a 28x28 image by computing the center of mass of the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at one sample in our training data\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 10 labels/classes of the training data\n",
    "print(y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the first 64 samples, and get a sense of the data\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(x_train[i],cmap=plt.cm.binary,interpolation='nearest')\n",
    "    ax.text(0, 7, str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the images into 1D arrays\n",
    "# Each image is currently a matrix with 28 rows and 28 columns, we can reshape this into a vector with (28x28) = 784 elements\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Convert the data type from int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize the data by dividing the maximum value\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the labeling look like now by printing out the first label\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ANN classifier  \n",
    "\n",
    "Let's start to build the ANN model. We start by initializing a sequential model and add two hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are adding and changing a few components of our MLP architecture.     \n",
    "1. `model.add(Dropout(0.2))` - Prevents our neural network from overfitting the data by ignoring (dropping) a random subset of neuron updates during backpropogation.     \n",
    "2. `activation = 'relu'` - We are now using the rectified linear activation unit (ReLU) which is more less computationally burdensome than the sigmoid.     \n",
    "3. `activation = 'softmax'` - A generalization of the sigmoid function returns a probability between 0-1 for all our classes and returns the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compile the model by telling it what algorithms we are using for the backprop. \n",
    "# what metrics to mesure and so on. \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During compiling we specify a few things.\n",
    "1. `loss='categorical_crossentropy'` - Define our loss function as cross entropy.\n",
    "2. `optimizer=RMSprop()` - Define our weight update optimizer as RMSprop (Root Mean Square Propogation).\n",
    "3. `metrics=['accuracy']` - Define our metrics to be displayed during model training. (This is for your own evaluation!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Let's train the model using a batch size 128 and 20 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are adding some additional arguments to our training process including:\n",
    "1. ` batch_size = 128 ` - The number of data samples we pass to the model each time we use to update the weights.\n",
    "2. ` epochs = 20 ` - The number of iterations use on the entire dataset. We are using 20 here, which means we run the training 20 times on all the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ANN classifier and evaluate  \n",
    "\n",
    "After we trained the ANN classifier, we will test the performance of the classifier using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 8)) \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results from the test data\n",
    "predicted = model.predict(x_test)\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "# plot the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test, axis =1),predicted)\n",
    "\n",
    "plot_confusion_matrix(cm, classes=np.unique(y_test),\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = np.argmax(y_test, axis =1) \n",
    "fig = plt.figure(figsize=(8, 8))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(x_test.reshape(-1, 28, 28)[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "\n",
    "    # label the image with the target value\n",
    "    if predicted[i] == expected[i]:\n",
    "        ax.text(0, 7, str(predicted[i]), color='green')\n",
    "    else:\n",
    "        ax.text(0, 7, str(predicted[i]), color='red')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpackages-ann",
   "language": "python",
   "name": "newpackages-ann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
