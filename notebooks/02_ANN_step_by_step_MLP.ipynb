{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we implemented a simple Perceptron algorithm and explained all the important parts of it. We concluded with a statement that, Perceptron has its own limitation which caused the winter of ANNs in the 1970s.   \n",
    "\n",
    "In short, the limitation is that the Perceptron algorithm only works on [linearly separable](https://en.wikipedia.org/wiki/Linear_separability) problems. For example, in the following figure, we have two cases. In each case, we have two classes with blue circles, and red triangles. The perceptron can solve the left problem without any issues (you can see the green line can separate the two classes very well). But on the right side, we can see that there is no single line that can be drawn to separate the two classes, hence, the perceptron algorithm fails to classify them.   \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/qingkaikong/blog/master/40_ANN_part3_step_by_step_MLP/figures/figure1_Perceptron_limitation.jpg\" width=\"600\"/> \n",
    "\n",
    "You may have already guessed that the solution to the above problem is today's topic - the Multi-Layer Perceptron (MLP), a very popular Artificial Neural Network algorithm that you will use it a lot. It turns out, to solve the above problem, all we need is adding one more layer between the input and output layer. As we talked before, this is the hidden layer, which can actually capture the non-linear relationship in the data, thus can solve the problem by drawing a non-linear boundary to classify the data. Of course, you can add more layers in between, but we will just add one layer for simplicity. (The field of adding more layers to model more combinations of relationships such as this is known as [\"deep learning\"](https://en.wikipedia.org/wiki/Deep_learning) because of the increasingly deep layers being modeled.)\n",
    "\n",
    "Let's use the same example we used last time, 5 training data samples, and each has 3 features as shown in the following table.   \n",
    "\n",
    "|Feature1|Feature2|Feature3|Target|\n",
    "|:------:|:------:|:------:|:----:|\n",
    "|    0   |    0   |    1   |   0  |\n",
    "|    1   |    1   |    1   |   1  |\n",
    "|    1   |    0   |    1   |   1  |\n",
    "|    0   |    1   |    1   |   0  |\n",
    "|    0   |    1   |    0   |   1  |\n",
    "\n",
    "But this time, we will build a MLP model to solve the same problem, i.e. we will add one hidden layer. In this newly added hidden layer, we have 4 nodes (neurons) which can be represented as the following:  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/qingkaikong/blog/master/40_ANN_part3_step_by_step_MLP/figures/figure2_MLP_structure.jpg\" width=\"600\"/>  \n",
    "\n",
    "We can see the structure of this Multilayer perceptron is more complicated than the simple perceptron. It has 3 layers\n",
    "1. An input layer\n",
    "2. A hidden layer\n",
    "3. An output layer\n",
    "\n",
    "The hidden layer has 4 nodes (neurons). The summation sign $\\sum$ and activation function $f$ in the neurons of the hidden layer indicate that each neuron will work as before: take the weighted sum of the input data and the weights, and pass this summation through our sigmoid activation function. The generated values from these hidden neurons are the outputs for the hidden layer, which in turn become the inputs for the output layer. Between each layer, we added the bias term to avoid the 0 input problem we described in the previous notebook. Overall, there are several more weights in the model. Each arrow in the figure is one weight, and we can see that, for each node in one layer, it will connect to all the nodes in the next layer. \n",
    "\n",
    "Since most of the important parts of the MLP are similar to the Perceptron model we discussed in the previous notebook, we only go over the major differences here. \n",
    "\n",
    "### Forward Propogation\n",
    "As before, the inputs propagate forward through the network to the output layer. We can think of each neuron in the hidden layer as a simple perceptron we talked before, with 4 copies of a parallel perceptron that not interact with each other, each with their own weights and biases as the following figure shows (note that I highlight the part that is essentially a perceptron structure). \n",
    "\n",
    "*note - unify language to say either neuron or node*\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/qingkaikong/blog/master/40_ANN_part3_step_by_step_MLP/figures/figure3_input_perceptron.jpg\" width=\"600\"/> \n",
    "\n",
    "After the neurons get the data form the input layers, they take the weighted sum and pass that to the sigmoid function which outputs a value between 0-1. The outputs from the hidden layer will be the input for the output layer, a strcuture analogous to the simple perceptron we previously built.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/qingkaikong/blog/master/40_ANN_part3_step_by_step_MLP/figures/figure4_hidden_perceptron.jpg\" width=\"600\"/> \n",
    "\n",
    "### Backprogation - Learning\n",
    "\n",
    "The learning or training of the MLP is more complicated, since now we have three layer with two sets of weights that need to be updated each time from the error. The method that we are going to use is called 'back-propagation', which makes it clear that the errors are sent backwards through the network. The best way to describe back-propagation properly is mathematically, but that is beyond the scope of this workshop. Instead, the purpose of this notebook is to show you a high level overview of how it works. If you want to learn more, check out this blog - [a step by step backpropagation example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/).  \n",
    "\n",
    "Let's look at the code to implement a simple version of MLP, and explain the lines that we didn't cover in the previous notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.52021\n",
      "Error:0.02142\n",
      "Error:0.00953\n",
      "Error:0.00641\n",
      "Error:0.00486\n",
      "Error:0.00392\n",
      "Output After Training:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Target\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define the sigmoid function\n",
    "def sigmoid(x,deriv=False):\n",
    "    sig = 1/(1+np.exp(-x))\n",
    "    if(deriv==True):\n",
    "        return sig*(1-sig)\n",
    "    return sig\n",
    "\n",
    "# define learning rate\n",
    "learning_rate = 0.4\n",
    "\n",
    "# the input data, and we add the bias in line 14\n",
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1],\n",
    "              [0,1,0]])\n",
    "X = np.concatenate((np.ones((len(X), 1)), X), axis = 1)\n",
    "\n",
    "y = np.array([[0],[1],[1],[0],[1]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0 for weights \n",
    "# connect input layer and hidden layer, and connect hidden\n",
    "# layer to output layer\n",
    "weights_0 = 2*np.random.random((4,4)) - 1\n",
    "weights_1 = 2*np.random.random((5,1)) - 1\n",
    "\n",
    "# training the model for 60000 iterations\n",
    "for j in range(60000):\n",
    "\n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    # input layer\n",
    "    layer_0 = X\n",
    "    # layer_1_output is the output from the hidden layer\n",
    "    layer_1_output = sigmoid(np.dot(layer_0,weights_0))\n",
    "    # Note here we add a bias term before we feed them into the output layer\n",
    "    layer_1_output = np.concatenate((np.ones((len(layer_1_output), 1)), layer_1_output), axis = 1)\n",
    "    \n",
    "    # layer_2_output is the estimation the model made using current weights\n",
    "    layer_2_output = sigmoid(np.dot(layer_1_output,weights_1))\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    layer2_error = y - layer_2_output\n",
    "    \n",
    "    # let's print out the error we made at each 10000 iteration\n",
    "    if (j% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(layer2_error)).round(5)))\n",
    "\n",
    "    # How much we will change for the weights connect hidden layer\n",
    "    # and output layer\n",
    "    layer2_delta = learning_rate*layer2_error*sigmoid(layer_2_output,deriv=True)\n",
    "\n",
    "    # how much did each hidden node value contribute to the output error (according to the weights)?\n",
    "    layer1_error = layer2_delta.dot(weights_1.T)\n",
    "\n",
    "    # How much we will change for the weights connect the input layer\n",
    "    # and the hidden layer\n",
    "    layer1_delta = learning_rate*layer1_error * sigmoid(layer_1_output,deriv=True)\n",
    "\n",
    "    # update the weights\n",
    "    weights_1 += layer_1_output.T.dot(layer2_delta)\n",
    "    weights_0 += layer_0.T.dot(layer1_delta[:, 1:])\n",
    "    \n",
    "print(\"Output After Training:\")\n",
    "print(layer_2_output.round(2))\n",
    "print('Target')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain line by line  \n",
    "\n",
    "Most of the lines are very similar to the previous perceptron model with the exception of lines 44 and 52.\n",
    "\n",
    "**Line 44:** we print the error at each 10,000th iteration (i.e. 10,000 , 20,000, ... , 60,000) with the expectation that the error will decrease as we update the weights. The more iterations we have, the smaller the error will be.  \n",
    "\n",
    "```python\n",
    "    # let's print out the error we made at each 10000 iteration\n",
    "    if (j% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(layer2_error))))\n",
    "```\n",
    "\n",
    "**Line 52:** uses the \"confidence weighted error\" from the output layer to establish an error for the hidden layer. We simply sends the error across the weights from output layer to hidden layer. This gives what you might call a \"contribution weighted error\" because we learn how much each node value in the hidden layer \"contributes\" to the error in output layer. We then update the weights that connect the input layer to the hidden layer using the same steps we did in the perceptron implementation.   \n",
    "\n",
    "```python\n",
    "    # how much did each hidden node value contribute to the output error (according to the weights)?\n",
    "    layer1_error = layer2_delta.dot(weights_1.T)\n",
    "```\n",
    "\n",
    "\n",
    "## Visualize ANN  \n",
    "Hopefully you have a better understanding of the ANN algorithm after reading these notebooks, which cover the most important concepts of the ANN! Luckily, we don't need to implement ANN from scratch, since there are already many packages out there for us to use. Next notebook, we will try to use the implementation of the [MLP in sklearn](http://scikit-learn.org/stable/modules/neural_networks_supervised.html) to classify the handwriting digits, while learn more about the Multi-Layer Perceptron that we haven't covered yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Acknowledgments\n",
    "\n",
    "[A Neural Network in 11 lines of Python](http://iamtrask.github.io/2015/07/12/basic-python-network/) (I thank the author, since my example is modified from his blog).    \n",
    "[Machine learning - An Algorithmic Perspective](https://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html)   \n",
    "[Single-Layer Neural Networks and Gradient Descent](http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
